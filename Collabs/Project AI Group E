{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project AI | Group E","provenance":[],"collapsed_sections":["oLwd6xF6Y7O-","_31zkPZnY9r0","_B8HboLUZATL","7PgULNVmZEZ8","NyLdHnmCZKfb","939AIkUUZNUs"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Import Part"],"metadata":{"id":"didD6aN8Yq8_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRJKFX4_-gu1"},"outputs":[],"source":["import tensorflow as tf \n","from tensorflow import keras \n","from tensorflow.keras import layers \n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","import matplotlib.pyplot as plt # used to plot the model graph \n","import numpy as np # used for math calculations\n","import os \n","from glob import glob # used to get all the files in a directory\n","from keras.preprocessing.image import ImageDataGenerator\n","from PIL import * # used for image manipulation\n","\n","#for loading and visualizing audio files\n","import librosa #import librosa\n","import librosa as lr\n","import librosa.display # display audio files\n","\n","from random import shuffle # for shuffling the data\n","\n","from pathlib import Path\n","import pathlib\n","import wave\n","import contextlib"]},{"cell_type":"code","source":["from google.colab import drive # Used to access Google Drive\n","drive.mount('/content/drive') # Mount Google Drive"],"metadata":{"id":"CqmFD2DC_3gL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Downloading the samples from the teacher"],"metadata":{"id":"FTRIGsoQYGTO"}},{"cell_type":"code","source":["!cd test_sample/wavFile/ && wget https://alg.backprop.fr/data4colab/sounds/en_01.wav \n","!cd test_sample/wavFile/ && wget https://alg.backprop.fr/data4colab/sounds/fr_01.wav"],"metadata":{"id":"x3VZ0OKI-qXU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Getting the prepared spectrograms from the drive"],"metadata":{"id":"M6p-rD1TY2TG"}},{"cell_type":"code","source":["##Getting the spectrograms from the drive\n","%%shell \n","cp ./drive/MyDrive/Spectrograms/train.zip /content/ # copy the train.zip file to the folder\n","cp ./drive/MyDrive/Spectrograms/test.zip /content/ # copy the test.zip file to the folder\n","\n","unzip -q '/content/train.zip' -d '/content/' && rm /content/train.zip  # unzip the train.zip file to the folder and remove the zip file\n","unzip -q '/content/test.zip' -d '/content/' && rm /content/test.zip  # unzip the test.zip file to the folder and remove the zip file"],"metadata":{"id":"p3dnQQF6AHmn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Global variables"],"metadata":{"id":"oLwd6xF6Y7O-"}},{"cell_type":"code","source":["img_width, img_height = 500, 128 # Setting the image size\n","batch_size = 32                  # batch size\n","epochs = 50                      # number of epochs\n","validation_split = 0.2           # validation split"],"metadata":{"id":"ftPqVQRF_LvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_files = glob('train/*/*.png') # Getting all the spectrograms from the train folder\n","\n","num_validation = len(all_files) * validation_split # Defining the number of validation files\n","num_train = len(all_files) - num_validation # Define the number of training images\n","\n","validation_steps = int(num_validation / batch_size) # Calculating the number of steps for the validation\n","steps_per_epoch = int(num_train / batch_size) # Calculating the number of steps for the training\n","\n","print('Steps per Epoch: ' + str(steps_per_epoch)) \n","print('Validation steps: ' + str(validation_steps))"],"metadata":{"id":"TIsDIwdCLVIX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train and Test definition"],"metadata":{"id":"_31zkPZnY9r0"}},{"cell_type":"code","source":["image_data_generator = ImageDataGenerator(validation_split=validation_split,rescale=1./255) # rescale to 0-1"],"metadata":{"id":"fgAN5oF3A3DL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_generator = image_data_generator.flow_from_directory( # train_data_generator\n","    './train/',\n","    target_size=(img_width, img_height), \n","    batch_size=batch_size, \n","    class_mode='binary', \n","    subset='training',\n","    color_mode = 'grayscale')\n","\n","validation_generator = image_data_generator.flow_from_directory( # validation_generator\n","    './train/',\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='binary',\n","    subset='validation',\n","    color_mode='grayscale')"],"metadata":{"id":"X4H6PGBE_QL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data_generator = image_data_generator.flow_from_directory( # test_data_generator\n","    './test/',\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='binary',\n","    color_mode='grayscale')"],"metadata":{"id":"b8dPXngfAVfe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model definition"],"metadata":{"id":"_B8HboLUZATL"}},{"cell_type":"code","source":["keras.backend.clear_session()"],"metadata":{"id":"qQTFijUHZd3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 1)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.GlobalMaxPool2D(),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])"],"metadata":{"id":"vdVdDyq7BWTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_accuracy',    # val_accuracy\n","    patience=8,                # patience before stop the train\n","    verbose=1,                 # verbose\n","    mode='max',                # mode\n","    restore_best_weights=True  # restore best weights\n",")"],"metadata":{"id":"DquBr9mHQCc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy',optimizer='adam',metrics='accuracy')"],"metadata":{"id":"kRu5yWQzBfSl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"cQ3OcvQKBicG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.output"],"metadata":{"id":"KqRUuB9cLmzu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fitting the model"],"metadata":{"id":"7PgULNVmZEZ8"}},{"cell_type":"code","source":["history = model.fit(\n","    train_data_generator,\n","    steps_per_epoch=steps_per_epoch,\n","    epochs=epochs,\n","    validation_data=validation_generator,\n","    validation_steps=validation_steps,\n","    callbacks=[early_stopping])"],"metadata":{"id":"UMELQiTAfRD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, accuracy = model.evaluate(test_data_generator)"],"metadata":{"id":"0LgCsx6WBrl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Accuracy of the model: ' + str(round(accuracy * 100., 1)) + '%')"],"metadata":{"id":"ikN9HOPeJpe_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plotting values from the model"],"metadata":{"id":"Qsbs5gkEZHDU"}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Accuracy for '+ str(epochs)+' epochs')\n","plt.ylabel('accuracy')\n","plt.xlabel('epochs')\n","plt.legend(['train', 'validation'], loc='lower right')\n","plt.show()"],"metadata":{"id":"G1FGHvT0CjbB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save the model"],"metadata":{"id":"NyLdHnmCZKfb"}},{"cell_type":"code","source":["!mkdir -p saved_model\n","model.save('saved_model/model_'+ str(round(accuracy * 100.))+'_accuracy', save_format='h5')"],"metadata":{"id":"lWBp_6sXhCEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir .testingWav\n","!cp ./drive/MyDrive/EN/TestSample/*.wav ./testingWav/"],"metadata":{"id":"xZzBOarBYpy0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Make prediction from the model"],"metadata":{"id":"939AIkUUZNUs"}},{"cell_type":"code","source":["global_files = glob(\"./*/*/*.png\") # get all files frrom the training folder and test folder\n","shuffle(global_files) # shuffle the dataset to randomize \n","random_file = global_files[0] # choose a random file from all the dataset\n","print(random_file)\n","\n","\n","img = tf.keras.utils.load_img((random_file), target_size=(500, 128),color_mode='grayscale') # load the image\n","img_array = tf.keras.utils.img_to_array(img)/255 # convert the image to an array\n","img_array = tf.expand_dims(img_array,0) # add a dimension to the array\n","\n","predict = model.predict(img_array) # Make a prediction on the random image\n","label = predict[0][0] # Get the value of the prediction\n","\n","# More is close to 0, more the prediction is an English conversation\n","# Less is close to 1, more the prediction is a French conversation\n","print('Value: ' + str(label)) \n","\n","if(label <= 0.25): \n","  print(\"You're speaking mostly in English\")\n","elif(label > 0.75): \n","  print(\"You're speaking mostly in French\") \n","elif(label > 0.25 and label <= 0.50): \n","  print(\"It's look like you're speaking in English\")\n","elif(label > 0.50 and label <= 0.75): \n","  print(\"It's look like you're speaking in French\")"],"metadata":{"id":"pPd26mzARw-1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Other functions"],"metadata":{"id":"AvVo0VDJaxZs"}},{"cell_type":"code","source":["!mkdir saved_model\n","!cp ./drive/MyDrive/Models/*.h5 ./saved_model/"],"metadata":{"id":"ujfiI_sTbe2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p ./test_sample/wavFile/ ./test_sample/pngFile/\n","!cp ./drive/MyDrive/EN/TestSample/*.wav ./test_sample/wavFile/"],"metadata":{"id":"JeFz9N6xjPvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getResult(label):\n","  if(label <= 0.25): \n","    percentage = 100 - (label * 100)\n","    result = 'You\\'re speaking mostly in English at ' + str(round(percentage, 3)) +'%'\n","  elif(label > 0.75): \n","    percentage = label * 100\n","    result = 'You\\'re speaking mostly in French at ' + str(round(percentage, 3)) +'%'\n","  elif(label > 0.25 and label <= 0.50):\n","    percentage = 100 - (label * 100)\n","    result = 'It\\'s look like you\\'re speaking in English at '  + str(round(percentage, 3)) +'%'\n","  elif(label > 0.50 and label <= 0.75):\n","    percentage = label * 100\n","    result = 'It\\'s look like you\\'re speaking in French at ' + str(round(percentage, 3)) +'%'\n","  return result"],"metadata":{"id":"rW-7ohircM7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def createSpectrogram(audio_segment):\n","    \n","    h1 = audio_segment.shape[0] // img_width\n","    spec = lr.feature.melspectrogram(audio_segment, n_mels=img_height, hop_length=int(h1))\n","\n","    image = lr.core.power_to_db(spec)\n","\n","    image_np = np.asmatrix(image)\n","\n","    image_np_scaled_temp = (image_np - np.min(image_np))\n","\n","    image_np_scaled = image_np_scaled_temp / np.max(image_np_scaled_temp)\n","\n","    return image_np_scaled[:, 0:img_width]"],"metadata":{"id":"Kj7jRrkwhzPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def makeSpectrogramOfFolder(path_folder):\n","  # Define Global variables\n","\n","  directorySample = path_folder\n","  outputSample = \"./test_sample/pngFile/\"\n","\n","  directories = [directorySample]\n","  outputs = [outputSample]\n","\n","  for directory in directories:\n","    for filename in os.listdir(directory):\n","      filetitle, _ = os.path.splitext(filename)\n","      audio_segment, sample_rate = librosa.load(directory+filename)\n","      a = createSpectrogram(audio_segment)\n","      for i in range(0,len(outputs)):\n","        if directory == directories[i]:\n","          plt.imsave(outputs[i]+filetitle+\".png\", a, cmap='gray')"],"metadata":{"id":"1qQR31QDifw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def makeSpectrogramOfFile(file_path):\n","  # Define Global variables\n","  filename = './test_sample/wavFile/en_sample_n3.wav'\n","  output = \"./test_sample/pngFile/\"\n","\n","  filetitle = Path(file_path).stem\n","  audio_segment, sample_rate = librosa.load(file_path)\n","  a = createSpectrogram(audio_segment)\n","  path = plt.imsave(output+filetitle+\".png\", a, cmap='gray')\n","\n","  png_path = output+filetitle+\".png\"\n","  return png_path"],"metadata":{"id":"Ghi7G2DKk8g8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predictRandomImg(path_model):\n","  loaded_model = tf.keras.models.load_model(path_model)\n","  \n","  global_files = glob(\"./*/*/*.png\") # get all files frrom the training folder and test folder\n","  shuffle(global_files) # shuffle the dataset to randomize \n","  random_file = global_files[0] # choose a random file from all the dataset\n","\n","  img = tf.keras.utils.load_img((random_file), target_size=(500, 128),color_mode='grayscale') # load the image\n","  img_array = tf.keras.utils.img_to_array(img)/255 # convert the image to an array\n","  img_array = tf.expand_dims(img_array,0) # add a dimension to the array\n","\n","  predict = loaded_model.predict(img_array) # Make a prediction on the random image\n","  label = predict[0][0] # Get the value of the prediction\n","  \n","  return getResult(label) "],"metadata":{"id":"xYgaqkXCZvtM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predictSpecificImg(image, path_model):\n","  loaded_model = tf.keras.models.load_model(path_model)\n","\n","  img = tf.keras.utils.load_img((image), target_size=(500, 128),color_mode='grayscale') # load the image\n","  img_array = tf.keras.utils.img_to_array(img)/255 # convert the image to an array\n","  img_array = tf.expand_dims(img_array,0) # add a dimension to the array\n","\n","  predict = loaded_model.predict(img_array) # Make a prediction on the random image\n","  label = predict[0][0] # Get the value of the prediction\n","  return getResult(label)"],"metadata":{"id":"GsDSJXxdaCgl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predictFromWav(wav_path, path_model):\n"," return predictSpecificImg(makeSpectrogramOfFile(wav_path), path_model)"],"metadata":{"id":"zofptKB1aKH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split a wav file of X seconds into smaller wav files of 8 seconds\n","def split_wavFile(wav_path, seconds):\n","    wav_file = wave.open(wav_path, 'r')     # open the wav file\n","    frames = wav_file.getnframes()          # get the number of frames\n","    rate = wav_file.getframerate()          # get the frame rate\n","    duration = frames / float(rate)         # get the duration\n","\n","    if duration < seconds:\n","        error = 'Cannot split the wav file because it is less than 8 seconds, please choose an another wavFile'\n","        return error\n","\n","    elif duration > seconds:\n","      num_chunks = int(duration / seconds)    # get the number of chunks\n","      filetitle = Path(wav_path).stem        # get the file title    \n","\n","      parent_folder = str(pathlib.Path(wav_path).parent.resolve()) + '/'\n","      folder_name = str(Path(wav_path).stem) + '_Splitted/' \n","      os.mkdir(parent_folder + folder_name)\n","\n","      for i in range(num_chunks):             # iterate over the number of chunks\n","          start = i * seconds * rate          # get the start frame\n","          end = (i + 1) * seconds * rate      # get the end frame\n","          wav_file.setpos(start)              # set the position in the file\n","          frames = wav_file.readframes(int(end - start))          # read the frames\n","          out_file = wave.open(parent_folder + str(folder_name) + str(filetitle) + '_' + str(i) + '.wav', 'w')    # open the output file\n","          out_file.setparams(wav_file.getparams())                # set the output file parameters\n","          out_file.writeframes(frames)        # write the frames to the output file\n","          out_file.close()                    # close the output file\n","      wav_file.close()                        # close the input file"],"metadata":{"id":"nOb74sVCxT-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getWavLength(wav_path):\n","  with contextlib.closing(wave.open(wav_path,'r')) as f:\n","    frames = f.getnframes()\n","    rate = f.getframerate()\n","    duration = frames / float(rate)\n","    result = round(duration,2)\n","    s = 'File: ' + str(wav_path) + ' |  Length: ' + str(result)\n","    return s + 's'"],"metadata":{"id":"3z7gwTTsxzBx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testing the functions"],"metadata":{"id":"kiiDFzzPb7_j"}},{"cell_type":"markdown","source":["Make all the spectrograms of the wavfolder"],"metadata":{"id":"qKZ16rMSCIPX"}},{"cell_type":"code","source":["#makeSpectrogramOfFolder('./test_sample/wavFile/en_01_Splitted/')"],"metadata":{"id":"4tP4JLVsjj4b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Predict a random image from the global dataset"],"metadata":{"id":"GMYny02xCPxJ"}},{"cell_type":"code","source":["#predictRandomImg('./saved_model/model_99_accuracy.h5')"],"metadata":{"id":"til30E8jOBhZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Predict a specific image "],"metadata":{"id":"_NjPa4VeCTSh"}},{"cell_type":"code","source":["#predictSpecificImg('./test_sample/pngFile/yes.png', './saved_model/model_99_accuracy.h5')"],"metadata":{"id":"bsOK-3bPhcVb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Predict from a wavFile"],"metadata":{"id":"80uN34mbCaAw"}},{"cell_type":"code","source":["#predictFromWav('./test_sample/wavFile/en_01_Splitted/en_01_1.wav', './saved_model/model_99_accuracy.h5')"],"metadata":{"id":"sFkTvTHkkbaL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Splitting the Wav file if it exceed 8 sec"],"metadata":{"id":"7u8rumDIZMA-"}},{"cell_type":"code","source":["#split_wavFile('./test_sample/wavFile/en_01.wav', 8)"],"metadata":{"id":"Aq4Z-6MZyMk6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Getting the length of the wavFile"],"metadata":{"id":"PjMOZAYxZnxB"}},{"cell_type":"code","source":["#getWavLength('./test_sample/wavFile/en_01.wav')"],"metadata":{"id":"iunRfGwkAOff"},"execution_count":null,"outputs":[]}]}